{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb980fb-7ad5-46df-9377-72a71b33970c",
   "metadata": {},
   "source": [
    "\n",
    "# Models\n",
    "\n",
    "In practice the code provided allows you to analyse any model built using the BERT architecture configuration. Feel free to look around on HuggingFace and find other models that the ones itemised below. The models below have been tested to confirmt ehy work with the code - if you try to load a model incorrectly configured for the analyser it will throw an error when you run any analysis.\n",
    "\n",
    "### Mono-Lingual Models\n",
    "\n",
    "these are models intended to be used for a single language. However because they are trained on data scraped from the internet they likely will have seen a substantive portion of data from other languages too.\n",
    "\n",
    "- BERT Base (english): https://huggingface.co/google-bert/bert-base-cased\n",
    "- BERT Spanish: https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased\n",
    "- BERT Finnish: https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1\n",
    "- BERT German: https://huggingface.co/dbmdz/bert-base-german-cased\n",
    "\n",
    "\n",
    "### Multi-Lingual Models\n",
    "\n",
    "-DistilBERT Multilingual: https://huggingface.co/distilbert/distilbert-base-multilingual-cased\n",
    "\n",
    "\n",
    "### Bi/Tri Lingual Models\n",
    "\n",
    "Work has extracted subsets of multi-lingual models that handle only a couple of lanuguages, rather than the 104 included in wikipedia data. Take a look at the paper that introduces this technique [here](https://aclanthology.org/2020.sustainlp-1.16.pdf). The resulting models may not be perfect analogues for bilinguals, as they are distiallations of a larger model. Here we list the distilled models as they're smaller than full-sized BERT and so easier to run locally.\n",
    "\n",
    "- BERT English/German: https://huggingface.co/Geotrend/distilbert-base-en-de-cased\n",
    "- BERT English/French: https://huggingface.co/Geotrend/distilbert-base-en-fr-cased\n",
    "- BERT English/French/German: https://huggingface.co/Geotrend/distilbert-base-en-fr-de-cased\n",
    "- BERT English/Spanish: https://huggingface.co/Geotrend/distilbert-base-en-es-cased\n",
    "- BERT English/French/Spanish: https://huggingface.co/Geotrend/distilbert-base-en-fr-es-cased\n",
    "\n",
    "\n",
    "## Historical Language Models\n",
    "\n",
    "These are models from the Bavarian State Library, trained on European Newspaper data from largely from 1850-1950, but ranging from 1600-1999 - more details about the dataset can be found on the model pages or [here](http://www.europeana-newspapers.eu/). They release both monolingual models, and different sizes of models trained multi-lingually.\n",
    "\n",
    "#### Mono-Lingual\n",
    "\n",
    "\n",
    "- Historical BERT French: https://huggingface.co/dbmdz/bert-base-french-europeana-cased\n",
    "- Historical BERT German: https://huggingface.co/dbmdz/bert-base-german-europeana-cased\n",
    "- Historical BERT Swedish: https://huggingface.co/dbmdz/bert-base-swedish-europeana-cased\n",
    "- Historical BERT Finnish: https://huggingface.co/dbmdz/bert-base-finnish-europeana-cased\n",
    "\n",
    "#### Multi-Lingual\n",
    "\n",
    "Below are 5 Different Sizes of a Multi-Lingual Historical BERT model. \n",
    "- Tiny: https://huggingface.co/dbmdz/bert-tiny-historic-multilingual-cased\n",
    "- Mini: https://huggingface.co/dbmdz/bert-mini-historic-multilingual-cased\n",
    "- Small: https://huggingface.co/dbmdz/bert-small-historic-multilingual-cased\n",
    "- Medium: https://huggingface.co/dbmdz/bert-medium-historic-multilingual-cased\n",
    "- Base: https://huggingface.co/dbmdz/bert-base-historic-multilingual-cased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06441ce3",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "We've prepared a number of datasets for you to use in analysis. Some others on HuggingFace will work out of the box with the existent code, if not a ```NotImplemented``` error will be thrown. If you would like to use a dataset that isn't supported you just need to write a new ```get_example``` function in the ```codebase/h/analysis.py ln118``` which formats the downloaded dataset so that it can be fed into the model. Alternatively call me (Henry) over, and I should be able to add support for any datasets you're interested in.\n",
    "\n",
    "## EuroParl\n",
    "\n",
    "The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It contains high-quality professional translations in 21 European languages. However is domain specific to parlimentary proceedings. This dataset has been a major part of training Machine Translation systems since 2006.\n",
    "\n",
    "Included Languages in the full dataset: Romanic (French, Italian, Spanish, Portuguese, Romanian), Germanic (English, Dutch, German, Danish, Swedish), Slavik (Bulgarian, Czech, Polish, Slovak, Slovene), Finni-Ugric (Finnish, Hungarian, Estonian), Baltic (Latvian, Lithuanian), and Greek.\n",
    "\n",
    "### Monolingual Data\n",
    "\n",
    "The same 100k sentences are sampled from 7 Languages\n",
    "\n",
    "- English: https://huggingface.co/datasets/hcoxec/english_100k\n",
    "- French: https://huggingface.co/datasets/hcoxec/french_100k\n",
    "- German: https://huggingface.co/datasets/hcoxec/german_100k\n",
    "- Finnish: https://huggingface.co/datasets/hcoxec/finnish_100k\n",
    "- Romanian: https://huggingface.co/datasets/hcoxec/romanian_100k\n",
    "- Danish: https://huggingface.co/datasets/hcoxec/danish_100k\n",
    "- Spanish: https://huggingface.co/datasets/hcoxec/spanish_100k\n",
    "\n",
    "### Multi-lingual Data\n",
    "\n",
    "The same 50k sentences are sampled from a pair of languages, and labelled with their langauge ID. This allows you to see the degree to which a model encodes the same sentences in different langauges differently.\n",
    "\n",
    "- Spanish/Danish: https://huggingface.co/datasets/hcoxec/danish_spanish_mix\n",
    "- Spanish/Romanian: https://huggingface.co/datasets/hcoxec/romanian_spanish_mix\n",
    "- Spanish/Finnish: https://huggingface.co/datasets/hcoxec/finnish_spanish_mix\n",
    "- Finnish/Danish: https://huggingface.co/datasets/hcoxec/finnish_danish_mix\n",
    "- Finnish/Romanian: https://huggingface.co/datasets/hcoxec/finnish_romanian_mix\n",
    "- Spanish/French: https://huggingface.co/datasets/hcoxec/french_spanish_mix\n",
    "- Spanish/German: https://huggingface.co/datasets/hcoxec/german_spanish_mix\n",
    "- German/Danish: https://huggingface.co/datasets/hcoxec/german_danish_mix\n",
    "- German/Romanian: https://huggingface.co/datasets/hcoxec/german_romanian_mix\n",
    "- German/Finnish: https://huggingface.co/datasets/hcoxec/german_finnish_mix\n",
    "- French/Danish: https://huggingface.co/datasets/hcoxec/french_danish_mix\n",
    "- French/Romanian: https://huggingface.co/datasets/hcoxec/french_romanian_mix\n",
    "- French/Finnish: https://huggingface.co/datasets/hcoxec/french_finnish_mix\n",
    "- French/German: https://huggingface.co/datasets/hcoxec/french_german_mix\n",
    "\n",
    "\n",
    "## GLUE Benchmark\n",
    "\n",
    "GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n",
    "\n",
    "It contains 12 subtasks, all of which are supported by the analysis code. View the full dataset here: https://huggingface.co/datasets/nyu-mll/glue\n",
    "\n",
    "They can be loaded using repo name ```nyu-mll/glue``` followed by the subtask name\n",
    "\n",
    "- ax A manually-curated evaluation dataset for fine-grained analysis of system performance on a broad range of linguistic phenomena. This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems. Use a model trained on MulitNLI to produce predictions for this dataset.\n",
    "\n",
    "- cola The Corpus of Linguistic Acceptability consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence.\n",
    "\n",
    "- mnli The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. The authors of the benchmark use the standard test set, for which they obtained private labels from the RTE authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) section. They also uses and recommend the SNLI corpus as 550k examples of auxiliary training data.\n",
    "\n",
    "- mnli_matched The matched validation and test splits from MNLI. See the \"mnli\" BuilderConfig for additional information.\n",
    "\n",
    "- mnli_mismatched The mismatched validation and test splits from MNLI. See the \"mnli\" BuilderConfig for additional information.\n",
    "\n",
    "- mrpc The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.\n",
    "\n",
    "- qnli The Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The authors of the benchmark convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue.\n",
    "\n",
    "- qqp The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.\n",
    "\n",
    "- rte The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. The authors of the benchmark combined the data from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli et al., 2009). Examples are constructed based on news and Wikipedia text. The authors of the benchmark convert all datasets to a two-class split, where for three-class datasets they collapse neutral and contradiction into not entailment, for consistency.\n",
    "\n",
    "- sst2 The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. It uses the two-way (positive/negative) class split, with only sentence-level labels.\n",
    "\n",
    "- stsb The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5.\n",
    "\n",
    "- wnli The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. The examples are manually constructed to foil simple statistical methods: Each one is contingent on contextual information provided by a single word or phrase in the sentence. To convert the problem into sentence pair classification, the authors of the benchmark construct sentence pairs by replacing the ambiguous pronoun with each possible referent. The task is to predict if the sentence with the pronoun substituted is entailed by the original sentence. They use a small evaluation set consisting of new examples derived from fiction books that was shared privately by the authors of the original corpus. While the included training set is balanced between two classes, the test set is imbalanced between them (65% not entailment). Also, due to a data quirk, the development set is adversarial: hypotheses are sometimes shared between training and development examples, so if a model memorizes the training examples, they will predict the wrong label on corresponding development set example. As with QNLI, each example is evaluated separately, so there is not a systematic correspondence between a model's score on this task and its score on the unconverted original task. The authors of the benchmark call converted dataset WNLI (Winograd NLI)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
